{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e246ef6",
   "metadata": {},
   "source": [
    "# Dr. X's Publications Analyzer - Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be27bf3",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f9b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# File handling\n",
    "import docx\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Vector DB\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "\n",
    "# NLP and embeddings\n",
    "import nltk\n",
    "from tiktoken import get_encoding\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# LLM interaction\n",
    "import ollama\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Evaluation \n",
    "from rouge import Rouge\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize tokenizer\n",
    "cl100k_tokenizer = get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788f91e",
   "metadata": {},
   "source": [
    "## Define system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722928e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRANSLATION_PROMPT = \"\"\"\n",
    "You are a professional translator. Translate the following text from {source_language} to {target_language}.\n",
    "Maintain the original structure, formatting, and technical terminology as much as possible.\n",
    "Here is the text to translate:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "SUMMARIZATION_PROMPT = \"\"\"\n",
    "You are an expert summarizer. Create a concise summary of the following text that captures the main ideas,\n",
    "key findings and important details. The summary should be about {summary_length} of the original text.\n",
    "\n",
    "Text to summarize:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "QA_PROMPT = \"\"\"\n",
    "You are an AI assistant tasked with providing detailed answers based solely on the given context. Your goal is to analyze the information provided and formulate a comprehensive, well-structured response to the question.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Previous Question (if any): {previous_question}\n",
    "Previous Answer (if any): {previous_answer}\n",
    "\n",
    "To answer the question:\n",
    "1. Thoroughly analyze the context, identifying key information relevant to the question.\n",
    "2. Take into account the previous question and answer if available to maintain conversation coherence.\n",
    "3. Organize your thoughts and plan your response to ensure a logical flow of information.\n",
    "4. Formulate a detailed answer that directly addresses the question, using only the information provided in the context.\n",
    "5. If the context doesn't contain sufficient information to fully answer the question, state this clearly in your response.\n",
    "\n",
    "Important: Base your entire response solely on the information provided in the context. Do not include any external knowledge or assumptions not present in the given text.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c64a2",
   "metadata": {},
   "source": [
    "## Define the TokenCounter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc36f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenCounter:\n",
    "    \"\"\"Utility class to count tokens and measure processing speed.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def start_counting(self):\n",
    "        \"\"\"Start the timer for performance measurement.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def add_tokens(self, text: str):\n",
    "        \"\"\"Count tokens in the provided text.\"\"\"\n",
    "        tokens = cl100k_tokenizer.encode(text)\n",
    "        self.token_count += len(tokens)\n",
    "        \n",
    "    def end_counting(self) -> dict:\n",
    "        \"\"\"End timing and return performance metrics.\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        elapsed_time = self.end_time - self.start_time\n",
    "        tokens_per_second = self.token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"elapsed_time_seconds\": elapsed_time,\n",
    "            \"total_tokens\": self.token_count,\n",
    "            \"tokens_per_second\": tokens_per_second\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149482e5",
   "metadata": {},
   "source": [
    "## Define the TextExtractor class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62634c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \"\"\"Extract text from various file formats.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_docx(file_content: bytes) -> str:\n",
    "        \"\"\"Extract text from a .docx file.\"\"\"\n",
    "        doc = docx.Document(BytesIO(file_content))\n",
    "        full_text = []\n",
    "        \n",
    "        # Extract text from paragraphs\n",
    "        for para in doc.paragraphs:\n",
    "            full_text.append(para.text)\n",
    "            \n",
    "        # Extract text from tables\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                row_text = []\n",
    "                for cell in row.cells:\n",
    "                    row_text.append(cell.text)\n",
    "                full_text.append(\" | \".join(row_text))\n",
    "                \n",
    "        return \"\\n\".join(full_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_pdf(file_content: bytes) -> List[Dict[str, Union[str, int]]]:\n",
    "        \"\"\"Extract text from a PDF file with page numbers.\"\"\"\n",
    "        pdf_file = BytesIO(file_content)\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        pages = []\n",
    "        \n",
    "        for i, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            pages.append({\n",
    "                \"page_number\": i + 1,\n",
    "                \"content\": text\n",
    "            })\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_excel(file_content: bytes, file_extension: str) -> str:\n",
    "        \"\"\"Extract text from Excel files (.xlsx, .xls, .xlsm).\"\"\"\n",
    "        df = pd.read_excel(BytesIO(file_content), sheet_name=None)\n",
    "        full_text = []\n",
    "        \n",
    "        for sheet_name, sheet_df in df.items():\n",
    "            full_text.append(f\"Sheet: {sheet_name}\")\n",
    "            # Convert DataFrame to string representation\n",
    "            full_text.append(sheet_df.to_string(index=True, header=True))\n",
    "            \n",
    "        return \"\\n\\n\".join(full_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_csv(file_content: bytes) -> str:\n",
    "        \"\"\"Extract text from CSV files.\"\"\"\n",
    "        csv_file = BytesIO(file_content)\n",
    "        csv_reader = csv.reader(csv_file.read().decode('utf-8').splitlines())\n",
    "        rows = list(csv_reader)\n",
    "        \n",
    "        # Format CSV data as text\n",
    "        text_rows = []\n",
    "        for row in rows:\n",
    "            text_rows.append(\" | \".join(row))\n",
    "            \n",
    "        return \"\\n\".join(text_rows)\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_text(cls, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract text from a file based on its extension.\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "            \n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        if file_extension == '.docx':\n",
    "            return {\n",
    "                \"text\": cls.extract_from_docx(file_content),\n",
    "                \"pages\": [{\"page_number\": 1, \"content\": cls.extract_from_docx(file_content)}],\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        elif file_extension == '.pdf':\n",
    "            pages = cls.extract_from_pdf(file_content)\n",
    "            return {\n",
    "                \"text\": \"\\n\".join([page[\"content\"] for page in pages]),\n",
    "                \"pages\": pages,\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        elif file_extension in ['.xlsx', '.xls', '.xlsm']:\n",
    "            return {\n",
    "                \"text\": cls.extract_from_excel(file_content, file_extension),\n",
    "                \"pages\": [{\"page_number\": 1, \"content\": cls.extract_from_excel(file_content, file_extension)}],\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        elif file_extension == '.csv':\n",
    "            return {\n",
    "                \"text\": cls.extract_from_csv(file_content),\n",
    "                \"pages\": [{\"page_number\": 1, \"content\": cls.extract_from_csv(file_content)}],\n",
    "                \"source\": file_name\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef518ec",
   "metadata": {},
   "source": [
    "## Define the TextChunker class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34109585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextChunker:\n",
    "    \"\"\"Break down texts into smaller, manageable parts.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 400, \n",
    "                 chunk_overlap: int = 100,\n",
    "                 separators: List[str] = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]):\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = separators\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=separators\n",
    "        )\n",
    "        \n",
    "    def chunk_document(self, document: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Chunk a document into smaller parts using cl100k_base tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            document: Dictionary containing document text and metadata\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked documents with metadata\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_counter = 0\n",
    "        \n",
    "        # Process each page\n",
    "        for page in document[\"pages\"]:\n",
    "            page_number = page[\"page_number\"]\n",
    "            page_content = page[\"content\"]\n",
    "            \n",
    "            # Create a Document object for the text splitter\n",
    "            doc = Document(\n",
    "                page_content=page_content,\n",
    "                metadata={\n",
    "                    \"source\": document[\"source\"],\n",
    "                    \"page\": page_number\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Split the document\n",
    "            split_docs = self.text_splitter.split_documents([doc])\n",
    "            \n",
    "            # Add chunk number to each split document\n",
    "            for split_doc in split_docs:\n",
    "                chunk_counter += 1\n",
    "                split_doc.metadata[\"chunk\"] = chunk_counter\n",
    "                chunks.append({\n",
    "                    \"text\": split_doc.page_content,\n",
    "                    \"metadata\": split_doc.metadata\n",
    "                })\n",
    "                \n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a215c9",
   "metadata": {},
   "source": [
    "## Define the VectorDatabase class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eee842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"Create and manage vector database for document chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"./vector_db\", collection_name: str = \"dr_x_publications\"):\n",
    "        self.db_path = db_path\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Create embedding function using Ollama\n",
    "        self.embedding_function = OllamaEmbeddingFunction(\n",
    "            url=\"http://localhost:11434/api/embeddings\",\n",
    "            model_name=\"nomic-embed-text:latest\"\n",
    "        )\n",
    "        \n",
    "        # Initialize ChromaDB client\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        \n",
    "        # Get or create collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        self.token_counter = TokenCounter()\n",
    "        \n",
    "    def add_documents(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Add document chunks to the vector database.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of document chunks with text and metadata\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing performance metrics\n",
    "        \"\"\"\n",
    "        documents, metadatas, ids = [], [], []\n",
    "        \n",
    "        self.token_counter.start_counting()\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            text = chunk[\"text\"]\n",
    "            metadata = chunk[\"metadata\"]\n",
    "            \n",
    "            # Generate unique ID\n",
    "            chunk_id = f\"{metadata['source']}_{metadata['page']}_{metadata['chunk']}\"\n",
    "            \n",
    "            documents.append(text)\n",
    "            metadatas.append(metadata)\n",
    "            ids.append(chunk_id)\n",
    "            \n",
    "            self.token_counter.add_tokens(text)\n",
    "            \n",
    "        # Add chunks to collection\n",
    "        self.collection.upsert(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        return self.token_counter.end_counting()\n",
    "    \n",
    "    def query(self, query_text: str, n_results: int = 5) -> Tuple[List[str], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Query the vector database for relevant chunks.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The query text\n",
    "            n_results: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing lists of documents and their metadata\n",
    "        \"\"\"\n",
    "        self.token_counter.start_counting()\n",
    "        self.token_counter.add_tokens(query_text)\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        performance = self.token_counter.end_counting()\n",
    "        logger.info(f\"Query performance: {performance}\")\n",
    "        \n",
    "        return results[\"documents\"][0], results[\"metadatas\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090266d",
   "metadata": {},
   "source": [
    "## Define the LanguageModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962dd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"Interface with local LLMs for various NLP tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"llama3.2:latest\"):\n",
    "        self.model_name = model_name\n",
    "        self.token_counter = TokenCounter()\n",
    "        self.previous_question = None\n",
    "        self.previous_answer = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def generate_answer(self, context: str, question: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generate an answer to a question based on context.\n",
    "        \n",
    "        Args:\n",
    "            context: The context for answering the question\n",
    "            question: The question to answer\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing the answer and performance metrics\n",
    "        \"\"\"\n",
    "        self.token_counter.start_counting()\n",
    "        \n",
    "        # Add context and question to token count\n",
    "        self.token_counter.add_tokens(context)\n",
    "        self.token_counter.add_tokens(question)\n",
    "        \n",
    "        # Format prompt with previous Q&A if available\n",
    "        prompt = QA_PROMPT.format(\n",
    "            context=context,\n",
    "            question=question,\n",
    "            previous_question=self.previous_question if self.previous_question else \"\",\n",
    "            previous_answer=self.previous_answer if self.previous_answer else \"\"\n",
    "        )\n",
    "        \n",
    "        # Call Ollama\n",
    "        response = ollama.generate(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        \n",
    "        answer = response['response']\n",
    "        \n",
    "        # Update previous Q&A for context in next query\n",
    "        self.previous_question = question\n",
    "        self.previous_answer = answer\n",
    "        \n",
    "        # Count tokens in the response\n",
    "        self.token_counter.add_tokens(answer)\n",
    "        \n",
    "        return answer, self.token_counter.end_counting()\n",
    "    \n",
    "    def translate_text(self, text: str, source_language: str, target_language: str) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Translate text between languages.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to translate\n",
    "            source_language: Source language\n",
    "            target_language: Target language\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing translated text and performance metrics\n",
    "        \"\"\"\n",
    "        self.token_counter.start_counting()\n",
    "        \n",
    "        prompt = TRANSLATION_PROMPT.format(\n",
    "            source_language=source_language,\n",
    "            target_language=target_language,\n",
    "            text=text\n",
    "        )\n",
    "        \n",
    "        self.token_counter.add_tokens(prompt)\n",
    "        \n",
    "        # Call Ollama\n",
    "        response = ollama.generate(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        \n",
    "        translation = response['response']\n",
    "        self.token_counter.add_tokens(translation)\n",
    "        \n",
    "        return translation, self.token_counter.end_counting()\n",
    "    \n",
    "    def summarize_text(self, text: str, summary_ratio: float = 0.3) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Summarize text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to summarize\n",
    "            summary_ratio: Ratio of summary length to original text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing summary and performance metrics\n",
    "        \"\"\"\n",
    "        self.token_counter.start_counting()\n",
    "        \n",
    "        prompt = SUMMARIZATION_PROMPT.format(\n",
    "            text=text,\n",
    "            summary_length=f\"{int(summary_ratio * 100)}%\"\n",
    "        )\n",
    "        \n",
    "        self.token_counter.add_tokens(prompt)\n",
    "        \n",
    "        # Call Ollama\n",
    "        response = ollama.generate(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        \n",
    "        summary = response['response']\n",
    "        self.token_counter.add_tokens(summary)\n",
    "        \n",
    "        return summary, self.token_counter.end_counting()\n",
    "    \n",
    "    def reset_conversation_context(self):\n",
    "        \"\"\"Reset the conversation context.\"\"\"\n",
    "        self.previous_question = None\n",
    "        self.previous_answer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cfc3b",
   "metadata": {},
   "source": [
    "## Define the DocumentAnalyzer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809a7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentAnalyzer:\n",
    "    \"\"\"Main class for analyzing Dr. X's publications.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vector_db_path: str = \"./vector_db\",\n",
    "                 collection_name: str = \"dr_x_publications\",\n",
    "                 llm_model: str = \"llama3.2:latest\"):\n",
    "        \n",
    "        self.text_extractor = TextExtractor()\n",
    "        self.text_chunker = TextChunker()\n",
    "        self.vector_db = VectorDatabase(vector_db_path, collection_name)\n",
    "        self.language_model = LanguageModel(llm_model)\n",
    "        self.rouge = Rouge()\n",
    "        \n",
    "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a document and add it to the vector database.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing processing information\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing document: {file_path}\")\n",
    "        \n",
    "        # Extract text from document\n",
    "        document = self.text_extractor.extract_text(file_path)\n",
    "        \n",
    "        # Chunk the document\n",
    "        chunks = self.text_chunker.chunk_document(document)\n",
    "        \n",
    "        # Add chunks to vector database\n",
    "        performance = self.vector_db.add_documents(chunks)\n",
    "        \n",
    "        return {\n",
    "            \"file_path\": file_path,\n",
    "            \"chunks_created\": len(chunks),\n",
    "            \"performance\": performance\n",
    "        }\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process all supported documents in a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to the directory\n",
    "            \n",
    "        Returns:\n",
    "            List of processing information for each document\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Get all files in directory\n",
    "        file_paths = [os.path.join(directory_path, f) for f in os.listdir(directory_path) \n",
    "                     if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        \n",
    "        # Filter to supported file types\n",
    "        supported_extensions = ['.docx', '.pdf', '.csv', '.xlsx', '.xls', '.xlsm']\n",
    "        file_paths = [f for f in file_paths if os.path.splitext(f)[1].lower() in supported_extensions]\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                result = self.process_document(file_path)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"file_path\": file_path,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def answer_question(self, question: str, n_results: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            n_results: Number of most relevant chunks to use\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the answer and related information\n",
    "        \"\"\"\n",
    "        # Query vector database\n",
    "        documents, metadatas = self.vector_db.query(question, n_results)\n",
    "        \n",
    "        # Re-rank documents using cross-encoder if available\n",
    "        try:\n",
    "            relevant_documents, relevant_indices = self.rerank_documents(question, documents)\n",
    "            relevant_metadata = [metadatas[i] for i in relevant_indices]\n",
    "        except:\n",
    "            logger.warning(\"Cross-encoder reranking failed, using original ranking\")\n",
    "            relevant_documents = documents\n",
    "            relevant_metadata = metadatas\n",
    "        \n",
    "        # Join documents into context\n",
    "        context = \"\\n\\n\".join(relevant_documents)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer, performance = self.language_model.generate_answer(context, question)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"source_documents\": relevant_documents,\n",
    "            \"source_metadata\": relevant_metadata,\n",
    "            \"performance\": performance\n",
    "        }\n",
    "    \n",
    "    def rerank_documents(self, question: str, documents: List[str], top_k: int = 3) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Re-rank documents using CrossEncoder for more accurate relevance.\n",
    "        \n",
    "        Args:\n",
    "            question: The question\n",
    "            documents: List of documents to rank\n",
    "            top_k: Number of top documents to return\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing list of re-ranked documents and their indices\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return [], []\n",
    "            \n",
    "        # Create pairs of (question, document) for each document\n",
    "        pairs = [[question, doc] for doc in documents]\n",
    "        \n",
    "        # Load cross-encoder model\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=self.language_model.device)\n",
    "        \n",
    "        # Predict scores\n",
    "        scores = cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Get indices of top-k scoring documents\n",
    "        top_indices = scores.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Get top documents\n",
    "        top_documents = [documents[i] for i in top_indices]\n",
    "        \n",
    "        return top_documents, top_indices.tolist()\n",
    "    \n",
    "    def translate_document(self, file_path: str, source_language: str, target_language: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Translate a document from one language to another.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document\n",
    "            source_language: Source language\n",
    "            target_language: Target language\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the translated document and performance metrics\n",
    "        \"\"\"\n",
    "        # Extract text from document\n",
    "        document = self.text_extractor.extract_text(file_path)\n",
    "        \n",
    "        translated_pages = []\n",
    "        total_performance = {\"elapsed_time_seconds\": 0, \"total_tokens\": 0, \"tokens_per_second\": 0}\n",
    "        \n",
    "        # Translate each page\n",
    "        for page in document[\"pages\"]:\n",
    "            translated_text, performance = self.language_model.translate_text(\n",
    "                page[\"content\"], source_language, target_language\n",
    "            )\n",
    "            \n",
    "            translated_pages.append({\n",
    "                \"page_number\": page[\"page_number\"],\n",
    "                \"content\": translated_text\n",
    "            })\n",
    "            \n",
    "            # Accumulate performance metrics\n",
    "            total_performance[\"elapsed_time_seconds\"] += performance[\"elapsed_time_seconds\"]\n",
    "            total_performance[\"total_tokens\"] += performance[\"total_tokens\"]\n",
    "        \n",
    "        # Calculate average tokens per second\n",
    "        if total_performance[\"elapsed_time_seconds\"] > 0:\n",
    "            total_performance[\"tokens_per_second\"] = (\n",
    "                total_performance[\"total_tokens\"] / total_performance[\"elapsed_time_seconds\"]\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"original_file\": file_path,\n",
    "            \"source_language\": source_language,\n",
    "            \"target_language\": target_language,\n",
    "            \"translated_pages\": translated_pages,\n",
    "            \"performance\": total_performance\n",
    "        }\n",
    "    \n",
    "    def summarize_document(self, file_path: str, summary_ratio: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Summarize a document.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document\n",
    "            summary_ratio: Ratio of summary length to original text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the summary and evaluation metrics\n",
    "        \"\"\"\n",
    "        # Extract text from document\n",
    "        document = self.text_extractor.extract_text(file_path)\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = \"\\n\\n\".join([page[\"content\"] for page in document[\"pages\"]])\n",
    "        \n",
    "        # Generate summary\n",
    "        summary, performance = self.language_model.summarize_text(full_text, summary_ratio)\n",
    "        \n",
    "        # Evaluate using ROUGE\n",
    "        rouge_scores = self.rouge.get_scores(summary, full_text)\n",
    "        \n",
    "        return {\n",
    "            \"original_file\": file_path,\n",
    "            \"summary\": summary,\n",
    "            \"rouge_scores\": rouge_scores[0],\n",
    "            \"performance\": performance\n",
    "        }\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset the conversation context in the language model.\"\"\"\n",
    "        self.language_model.reset_conversation_context()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb204e0",
   "metadata": {},
   "source": [
    "# Initialize the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c11089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:47:00,865 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyzer = DocumentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c191f5",
   "metadata": {},
   "source": [
    "## Process a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec0eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with your own file path\n",
    "document_path = \"Files/The_Plan_of_the_Giza_Pyramids.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e0c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:47:14,139 - INFO - Processing document: Files/The_Plan_of_the_Giza_Pyramids.pdf\n",
      "2025-04-13 18:47:17,431 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,455 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,476 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,501 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,529 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,557 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,577 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,598 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,616 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,644 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,664 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,685 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,704 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,728 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,745 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,764 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,783 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,802 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,822 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,841 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,861 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,880 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,899 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,918 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,935 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,953 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,973 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:17,990 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,009 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,032 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,051 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,076 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,095 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,114 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,135 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,154 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,174 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,193 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,210 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,229 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,247 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,268 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,288 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,308 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,327 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,345 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,363 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,382 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,400 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,419 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,438 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,457 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,476 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,500 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,518 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,539 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,558 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,576 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,593 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,611 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,627 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,645 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,663 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,685 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,702 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,719 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,736 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,755 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,776 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,800 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,829 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,853 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,878 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,903 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,924 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,949 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,972 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:18,991 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,011 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,032 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,052 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,073 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,093 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,113 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,133 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,153 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,171 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,189 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,215 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,236 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,259 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,280 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,310 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,333 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,352 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,372 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,393 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,413 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,434 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,453 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,472 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,490 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,507 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,528 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,546 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,567 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,585 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,605 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,628 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,645 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,662 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,679 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,697 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,716 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,733 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,753 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,772 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,791 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,809 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,826 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,846 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,865 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,882 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,899 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,917 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,935 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,954 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,972 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:19,989 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:20,009 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:20,027 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:20,043 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:47:20,062 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed document: Files/The_Plan_of_the_Giza_Pyramids.pdf\n",
      "Created 133 chunks\n",
      "Performance: {'elapsed_time_seconds': 6.028315544128418, 'total_tokens': 11439, 'tokens_per_second': 1897.5449968178243}\n"
     ]
    }
   ],
   "source": [
    "result = analyzer.process_document(document_path)\n",
    "print(f\"Processed document: {document_path}\")\n",
    "print(f\"Created {result['chunks_created']} chunks\")\n",
    "print(f\"Performance: {result['performance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f9773",
   "metadata": {},
   "source": [
    "## Process multiple documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775620f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Replace with your own directory path\u001b[39;00m\n\u001b[32m      2\u001b[39m directory_path = \u001b[33m\"\u001b[39m\u001b[33mFiles\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43manalyzer\u001b[49m.process_directory(directory_path)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[31mNameError\u001b[39m: name 'analyzer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace with your own directory path\n",
    "directory_path = \"Files\"\n",
    "\n",
    "\n",
    "results = analyzer.process_directory(directory_path)\n",
    "print(f\"Processed {len(results)} documents\")\n",
    "for result in results:\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error processing {result['file_path']}: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Processed {result['file_path']}: {result['chunks_created']} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c475e8",
   "metadata": {},
   "source": [
    "## Ask questions about the processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "321b0189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:09:07,056 - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 19:09:07,068 - INFO - Query performance: {'elapsed_time_seconds': 2.1565732955932617, 'total_tokens': 10, 'tokens_per_second': 4.636985916701271}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35673ea6fee475e85f829d916ffda64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 19:09:12,549 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: can glen get caught up in  stuff ?\n",
      "\n",
      "Answer: Based on the provided context, it appears that there is no direct information about Glen getting caught up in \"stupid stuff\" or any other topic related to personal behavior or interests. The text only discusses the surveys conducted by Mark Lehner and David Goodman as part of the Giza Plateau Mapping Project.\n",
      "\n",
      "Given this lack of relevant information, I can suggest a few possibilities:\n",
      "\n",
      "1. The question may be unrelated to the provided context, which focuses on historical archaeological surveys.\n",
      "2. The context does not provide enough information to answer the question, and further clarification or context would be needed to provide a meaningful response.\n",
      "3. It is possible that Glen Dash is an individual involved in the surveys or research being discussed, but without more context, it's difficult to say whether this is relevant to the question.\n",
      "\n",
      "In light of these possibilities, I must conclude that the context does not contain sufficient information to answer the question \"can glen get caught up in stuff?\" directly. Therefore, I will provide a response that acknowledges this limitation:\n",
      "\n",
      "I couldn't find any direct information or context within the provided text to address the question of whether Glen can get caught up in \"stuff.\" The text appears to focus on the surveys conducted by Mark Lehner and David Goodman as part of the Giza Plateau Mapping Project, discussing the triangulation method used by Petrie and a discrepancy in his measurements. If you could provide more context or clarify your question, I'd be happy to try and assist you further.\n",
      "\n",
      "Source documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Source: The_Plan_of_the_Giza_Pyramids.pdf, Page: 4, Chunk: 36\n",
      "12   Glen Dash, 'New Angles on the Great Pyramid' , Aeragram  13-2, Fall 2012, 10-19 \n",
      "13  See Petrie, Pyramids and Temples p. 205-7.  The shortfall ma...\n",
      "\n",
      "--- Document 2 ---\n",
      "Source: tmpweipkz8w.pdf, Page: 4, Chunk: 36\n",
      "12   Glen Dash, 'New Angles on the Great Pyramid' , Aeragram  13-2, Fall 2012, 10-19 \n",
      "13  See Petrie, Pyramids and Temples p. 205-7.  The shortfall ma...\n",
      "\n",
      "--- Document 3 ---\n",
      "Source: tmpweipkz8w.pdf, Page: 4, Chunk: 35\n",
      "of Petries triangulation was effectively lacking by about 1 part in 5,000  a discrepancy which \n",
      "Petrie himself noted when he compared his findings w...\n",
      "\n",
      "Performance: {'elapsed_time_seconds': 4.250937223434448, 'total_tokens': 531, 'tokens_per_second': 124.91363012201592}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"can glen get caught up in  stuff ?\"\n",
    "\n",
    "\n",
    "result = analyzer.answer_question(question, n_results=5)\n",
    "print(\"\\nQuestion:\", result[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])\n",
    "print(\"\\nSource documents:\")\n",
    "for i, (doc, meta) in enumerate(zip(result[\"source_documents\"], result[\"source_metadata\"])):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Source: {meta['source']}, Page: {meta['page']}, Chunk: {meta['chunk']}\")\n",
    "    print(doc[:150] + \"...\" if len(doc) > 150 else doc)\n",
    "print(\"\\nPerformance:\", result[\"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb83ac",
   "metadata": {},
   "source": [
    "# Translate a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5bbef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:49:42,782 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:49:49,830 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:02,182 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:15,654 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:30,633 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:41,989 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:50,528 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:50:56,788 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:05,245 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:10,638 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:20,951 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:26,717 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:32,822 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:40,297 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:51:53,280 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-04-13 18:52:22,919 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation from English to Arabic:\n",
      "\n",
      "--- Page 1 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "  John A.R. Legon \n",
      " : 22-11-2019 \n",
      "         piramides           S...\n",
      "\n",
      "--- Page 2 ---\n",
      "The translation is as follows:\n",
      "\n",
      "\n",
      "          jet     .  \n",
      "\n",
      "\n",
      "      orial                 ...\n",
      "\n",
      "--- Page 3 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "  : \n",
      "   3\n",
      "              bl I                 bl ...\n",
      "\n",
      "--- Page 4 ---\n",
      "Here is the translation of the provided text from English to Arabic:\n",
      "\n",
      "    \n",
      "\n",
      " 4 \n",
      "\n",
      "  side 1st   3rd_pyramid_35713.2 _1731.97       _1732 \n",
      " 1st   2nd_pyramid_5159.7 _250.23         _250 \n",
      " 2nd   3rd_pyramid_8856.1 _429.49 _429.5 \n",
      "\n",
      "...\n",
      "\n",
      "--- Page 5 ---\n",
      "Here is the translation of the provided text from English to Arabic:\n",
      "\n",
      " 5   5          tails     GPMP 14       . \n",
      " Koordinates          orientat...\n",
      "\n",
      "--- Page 6 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "     6 \n",
      " .    north-eastern    southwards and westwards    .               PYRAMID    ...\n",
      "\n",
      "--- Page 7 ---\n",
      "The translation is as follows:\n",
      "\n",
      "  pyramids  7 \n",
      " .       North-South axis   .             1101   0.1%     1100    2.5 times  ...\n",
      "\n",
      "--- Page 8 ---\n",
      "The translation of the given text from English to Arabic is as follows:\n",
      "\n",
      "     8 \n",
      "\n",
      "    pyramids   (8)    pyramid   (8). \n",
      "   pyramid    (440  1.5 - 250)  (410)     ...\n",
      "\n",
      "--- Page 9 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "    \n",
      "9\n",
      "             Two Three   1000.\n",
      " : \n",
      "\n",
      "                   10002 =1414.21...       10003 =1732.05...\n",
      "   ...\n",
      "\n",
      "--- Page 10 ---\n",
      "Here's the translation of the text from English to Arabic:\n",
      "\n",
      "   10 \n",
      "    2 : 3. .length marked off along 1 : 1 \n",
      "ijkstra: \n",
      "                           2 \n",
      "10003    --------     2 = 1101.020 \n",
      "                   2 + 3 \n",
      "   ...\n",
      "\n",
      "--- Page 11 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "  far 11 \n",
      "\n",
      "               .  \n",
      " Only the pyramid bases and the components of spacing between them had to be marked out on the ground of the Giza plateau. \n",
      "...\n",
      "\n",
      "--- Page 12 ---\n",
      "Here is the translation of the given text from English to Arabic:\n",
      "\n",
      "  pyramid 12\n",
      "    rectangle    250  250 2             250   353.5   .            1...\n",
      "\n",
      "--- Page 13 ---\n",
      "Here is the translation from English to Arabic:\n",
      "\n",
      "     13 \n",
      "\n",
      "                 250 2 by 250 5  ,  AGON     250 3 by 500 .     ...\n",
      "\n",
      "--- Page 14 ---\n",
      "Here is the translation of the given text from English to Arabic:\n",
      "\n",
      "\n",
      "     pyramid  \n",
      "    \n",
      "\n",
      "        (631  353.5)  277.5  \n",
      "       353.5      /4  pyramid . ...\n",
      "\n",
      "--- Page 15 ---\n",
      "Here is the translated text from English to Arabic:\n",
      "\n",
      "     15\n",
      "\n",
      "                     faktor 250 .           201.4...\n",
      "\n",
      "--- Page 16 ---\n",
      "Here is the translation of the text from English to Arabic:\n",
      "\n",
      "\n",
      "  16\n",
      "           thorie          imension             ...\n",
      "\n",
      "Performance: {'elapsed_time_seconds': 172.41396975517273, 'total_tokens': 29396, 'tokens_per_second': 170.49662531256732}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace with your own file path\n",
    "translate_file_path = \"Files/The_Plan_of_the_Giza_Pyramids.pdf\"\n",
    "source_language = \"English\"\n",
    "target_language = \"Arabic\"\n",
    "\n",
    "\n",
    "result = analyzer.translate_document(translate_file_path, source_language, target_language)\n",
    "print(f\"\\nTranslation from {source_language} to {target_language}:\")\n",
    "for page in result[\"translated_pages\"]:\n",
    "    print(f\"\\n--- Page {page['page_number']} ---\")\n",
    "    print(page[\"content\"][:300] + \"...\" if len(page[\"content\"]) > 300 else page[\"content\"])\n",
    "print(\"\\nPerformance:\", result[\"performance\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c183b",
   "metadata": {},
   "source": [
    "## Summarize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7574fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 21:09:35,928 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "This text is a lengthy and detailed discussion about the author's theories regarding the construction and purpose of the Great Pyramid of Giza, as well as the broader context of ancient Egyptian history.\n",
      "\n",
      "The author presents several key points:\n",
      "\n",
      "1. **Challenging conventional wisdom**: The author disputes the traditional view that the Great Pyramid was built during the reign of Pharaoh Khufu of the Fourth Dynasty. Instead, they propose that the pyramid may predate this dynasty.\n",
      "2. **Alternative purpose**: The author suggests that the pyramids were not primarily tombs for pharaohs but rather a center for the initiation of priest-neophytes into the occult wisdom of ancient Egypt.\n",
      "3. **Evidence from Egyptian records**: The author cites records preserved by the Egyptians themselves, such as the Turin King List and the history of Manetho, which mention an era known as Zep Tepi (the First Time), when Egypt was ruled by priest-initiates who founded the divine dynasties.\n",
      "4. **The Great Sphinx**: The author notes that the Great Sphinx is a monument from this era, carefully integrated with the Giza plan, and suggests that it may hold the key to understanding the true purpose of the pyramids.\n",
      "\n",
      "Throughout the text, the author draws on various sources, including Herodotus' history, to support their theories. They also offer alternative explanations for certain aspects of ancient Egyptian history and culture.\n",
      "\n",
      "The tone of the text is informative, speculative, and sometimes critical of conventional wisdom. The author presents a complex and unconventional narrative that challenges readers to consider alternative perspectives on one of history's greatest mysteries.\n",
      "\n",
      "It's worth noting that this text appears to be a draft or an early version of a book, as it includes references to other sources (e.g., \"H.C. Randall-Stevens\") and does not appear to have undergone significant editing or fact-checking.\n",
      "\n",
      "ROUGE Scores:\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"r\": 0.06316410861865407,\n",
      "    \"p\": 0.5721925133689839,\n",
      "    \"f\": 0.11376926987337208\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"r\": 0.0178027641133755,\n",
      "    \"p\": 0.2783882783882784,\n",
      "    \"f\": 0.03346543259977481\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"r\": 0.05726092089728453,\n",
      "    \"p\": 0.5187165775401069,\n",
      "    \"f\": 0.10313662766178251\n",
      "  }\n",
      "}\n",
      "\n",
      "Performance: {'elapsed_time_seconds': 6.214249134063721, 'total_tokens': 10004, 'tokens_per_second': 1609.8485567890364}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace with your own file path\n",
    "summarize_file_path = \"Files/The_Plan_of_the_Giza_Pyramids.pdf\"\n",
    "summary_ratio = 0.3  # 30% of original length\n",
    "\n",
    "\n",
    "result = analyzer.summarize_document(summarize_file_path, summary_ratio)\n",
    "print(\"\\nSummary:\")\n",
    "print(result[\"summary\"])\n",
    "+\n",
    "print(\"\\nROUGE Scores:\")\n",
    "print(json.dumps(result[\"rouge_scores\"], indent=2))\n",
    "print(\"\\nPerformance:\", result[\"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efc4b7",
   "metadata": {},
   "source": [
    "## Reset conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155512a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer.reset_conversation()\n",
    "print(\"Conversation context reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1083a",
   "metadata": {},
   "source": [
    "## Display performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fd9af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if hasattr(analyzer, \"performance_metrics\") and analyzer.performance_metrics:\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    metrics_df = pd.DataFrame(analyzer.performance_metrics)\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Plot tokens per second over time\n",
    "    if \"tokens_per_second\" in metrics_df.columns:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(metrics_df[\"tokens_per_second\"])\n",
    "        plt.title(\"Tokens Per Second\")\n",
    "        plt.xlabel(\"Operation\")\n",
    "        plt.ylabel(\"Tokens/Second\")\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa384b-c089-4215-b039-494045d18c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a44414-ea8d-4824-a709-c8e37db6044a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
